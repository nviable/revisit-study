<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Digital Visual Media Forensics Ontology</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/sigma.js/1.2.1/sigma.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/sigma.js/1.2.1/plugins/sigma.parsers.json.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/sigma.js/1.2.1/plugins/sigma.layout.forceAtlas2.min.js"></script>
    <script src="https://d3js.org/d3.v7.min.js"></script>

    <style>
        body {
            font-family: 'Inter', sans-serif;
            overscroll-behavior: none; 
        }
        .ontology-node {
            cursor: pointer;
            padding: 3px 0;
            font-size: 0.9rem;
        }
        .ontology-node:hover {
            background-color: #f0f0f0;
        }
        .node-children {
            margin-left: 15px;
            display: none; 
        }
        .node-expanded > .node-children {
            display: block;
        }
        .node-name {
            display: flex;
            align-items: center;
        }
        .toggle-icon {
            margin-right: 6px;
            width: 14px; 
            display: inline-block;
            text-align: center;
            font-size: 0.8rem;
        }
        .details-panel {
            border: 1px solid #ccc;
            padding: 15px;
            margin-top: 10px;
            background-color: #f9f9f9;
            border-radius: 8px;
            min-height: 250px; 
            display: flex;
            flex-direction: column;
        }
        .details-panel h3 {
            margin-top: 0;
            font-size: 1.15em;
            color: #333;
        }
        .details-panel p {
            margin-bottom: 8px;
            line-height: 1.5;
            font-size: 0.9rem;
        }
        .details-panel strong {
            color: #555;
        }
        .details-panel code {
            background-color: #e0e0e0;
            padding: 1px 3px;
            border-radius: 3px;
            font-family: monospace;
            font-size: 0.85rem;
        }
        .container-grid {
            display: grid;
            grid-template-columns: 1fr; 
            gap: 15px;
            height: calc(100vh - 180px);
        }
        @media (min-width: 768px) { 
            .container-grid {
                grid-template-columns: 1fr 1.3fr;
            }
        }
        .view-container {
            overflow: auto;
            border: 1px solid #d1d5db; 
            border-radius: 0.375rem; 
            padding: 0.75rem; 
            background-color: #f9fafb; 
            height: 100%; 
            position: relative; /* For tooltip positioning */
        }
        #sigma-graph-container, #sunburst-container {
            width: 100%;
            height: 100%;
        }

        .view-switcher {
            display: flex;
            justify-content: center;
            margin-bottom: 0.75rem;
        }
        .view-switcher button {
            padding: 0.4rem 0.8rem;
            margin: 0 0.2rem; 
            border: 1px solid #d1d5db; 
            background-color: #fff; 
            cursor: pointer;
            transition: background-color 0.2s ease, color 0.2s ease;
            font-size: 0.9rem;
        }
        .view-switcher button.active {
            background-color: #3b82f6; 
            color: white;
            border-color: #3b82f6; 
        }
        .view-switcher button:first-child {
            border-top-left-radius: 0.375rem; 
            border-bottom-left-radius: 0.375rem;
        }
        .view-switcher button:last-child {
            border-top-right-radius: 0.375rem; 
            border-bottom-right-radius: 0.375rem;
        }
        
        /* Sunburst specific styles */
        #sunburst-container svg {
            width: 100%;
            height: 100%;
            font-family: sans-serif;
            font-size: 10px;
        }
        #sunburst-container path {
            cursor: pointer;
            stroke: #fff;
            stroke-width: 0.5px;
        }
        #sunburst-container path:hover {
            opacity: 0.8;
        }
         #sunburst-tooltip {
            position: absolute;
            text-align: center;
            padding: 8px;
            font: 12px sans-serif;
            background: lightsteelblue;
            border: 0px;
            border-radius: 8px;
            pointer-events: none;
            opacity: 0;
            z-index: 10;
        }
    </style>
</head>
<body class="bg-gray-100 p-2 md:p-4">
    <div class="max-w-full mx-auto bg-white p-3 md:p-4 rounded-lg shadow-lg" style="height: calc(100vh - 2rem); display: flex; flex-direction: column;">
        <header class="mb-3">
            <h1 class="text-xl md:text-2xl font-bold text-center text-gray-800">Digital Visual Media Forensics Ontology</h1>
        </header>

        <div class="view-switcher">
            <button id="tree-view-btn" class="active">Tree View</button>
            <button id="graph-view-btn">Graph View</button>
            <button id="sunburst-view-btn">Sunburst View</button>
        </div>

        <div class="container-grid flex-grow">
            <div id="views-column" class="flex flex-col">
                <div id="ontology-tree-container" class="view-container">
                    <h2 class="text-lg font-semibold mb-2 text-gray-700 sticky top-0 bg-gray-50 py-1 z-10">Ontology Structure (Tree)</h2>
                </div>
                <div id="sigma-graph-wrapper" class="view-container" style="display: none;">
                     <h2 class="text-lg font-semibold mb-1 text-gray-700 sticky top-0 bg-gray-50 py-1 z-10">Ontology Structure (Graph)</h2>
                    <div id="sigma-graph-container"></div>
                </div>
                <div id="sunburst-wrapper" class="view-container" style="display: none;">
                     <h2 class="text-lg font-semibold mb-1 text-gray-700 sticky top-0 bg-gray-50 py-1 z-10">Ontology Structure (Sunburst)</h2>
                    <div id="sunburst-container"></div>
                    <div id="sunburst-tooltip"></div>
                </div>
            </div>

            <div id="details-panel" class="details-panel">
                <h3 id="details-name" class="font-semibold text-lg">Select a node</h3>
                <div id="details-content-static" class="overflow-y-auto">
                    <p>Click on any item in the ontology structure to view its reasoning and examples here.</p>
                </div>

            </div>
        </div>
    </div>

    <script>
        const ontologyData = {
            id: "0",
            name: "0. Digital-Visual-Media-Forensics Ontology",
            reasoning: "The root concept encompassing all aspects of analyzing digital visual media to determine authenticity, origin, or history. This refined ontology is based on a hierarchical structure focusing on modality, manipulation, features, and scope.",
            examples: "`Digital Forensics`, `Media Authenticity`, `Forgery Detection`",
            children: [
                {
                    id: "1", name: "1. Target Modality",
                    reasoning: "Categorizes the type of digital media being analyzed.",
                    examples: "`Multimedia contents`, `Digital media files`",
                    children: [
                        {
                            id: "1.1", name: "1.1 Image (still)",
                            reasoning: "Refers to static visual media.",
                            examples: "`Image`, `Images`, `Still Image`, `Digital Image`, `JPEG Image`, `PNG Image`, `Bitmap`, `RGB images`, `Face images from GANs`",
                            children: [
                                { id: "1.1.1", name: "1.1.1 Specialised-Images", reasoning: "Denotes image types from specific domains like remote sensing, healthcare, or microscopy.", examples: "`Satellite Images`, `Medical X-Ray`, `Microscopic Images`, `Astronomical Images`, `Scanned Document Images`, `Frontal face images`" }
                            ]
                        },
                        { id: "1.2", name: "1.2 Video", reasoning: "Refers to moving visual media, incorporating a temporal dimension.", examples: "`Video`, `Video files`, `Digital video files`, `Video frame`, `Video sequence`, `MP4 Video File`, `AVI Sequence`, `Live Stream`" },
                        { id: "1.3", name: "1.3 Multimodal (image + audio/text)", reasoning: "Pertains to techniques that involve multiple types of media simultaneously.", examples: "`Audio-visual data`, `Image-caption consistency check`, `Video with subtitles verification`, `Image-text pair`, `Audio and video segments`" },
                        { id: "1.4", name: "1.4 Text-only (rare; for cross-modal consistency)", reasoning: "Addresses textual data, often analyzed in conjunction with visual media for consistency.", examples: "`Image caption verification`, `Text-in-video analysis`, `Document metadata analysis`" }
                    ]
                },
                {
                    id: "2", name: "2. Manipulation / Task",
                    reasoning: "Describes the specific type of alteration a forensic method aims to detect or the overall forensic objective. This branch is organized by the nature of the manipulation, from creating new content to editing existing pixels to tampering with file structures.",
                    examples: "`Video manipulations`, `Image manipulation`, `Tampering`, `Image Forgery`, `Facial Manipulation`",
                    children: [
                        {
                            id: "2.1", name: "2.1 Synthetic Content Generation",
                            reasoning: "Tasks focused on detecting content that is partially or fully generated by AI/algorithmic processes. This is distinct from edits of camera-captured content.",
                            examples: "`Synthetic images`, `AI-generated faces`, `Fake image detection`",
                            children: [
                                {
                                    id: "2.1.1", name: "2.1.1 Full Synthesis (from scratch)",
                                    reasoning: "Detecting media that is entirely computer-generated without a direct photographic source image for the main subject.",
                                    examples: "`GAN-synthesised Images`, `Diffusion-generated images`, `Entire face synthesis`",
                                    children: [
                                        { id: "2.1.1.1", name: "2.1.1.1 GAN-Based Generation", examples: "`StyleGAN2`, `ProGAN`" },
                                        { id: "2.1.1.2", name: "2.1.1.2 Diffusion-Based Generation", examples: "`Images from LTIMs`, `DDPM-generated images`" },
                                        { id: "2.1.1.3", name: "2.1.1.3 Other CGI", examples: "`Distinguishing CGI from Photographic Images`" }
                                    ]
                                },
                                {
                                    id: "2.1.2", name: "2.1.2 Deepfake (AI-Mediated Reenactment & Swapping)",
                                    reasoning: "Detecting human-centric manipulations where an existing person's likeness or voice is altered or swapped using deep learning.",
                                    examples: "`Deepfake`, `Face Forgery`, `Facial Reenactment`, `Face2Face (F2F)`",
                                    children: [
                                        { id: "2.1.2.1", name: "2.1.2.1 Face / Identity Swap", examples: "`Face-swap deep fake`, `FaceSwap (FS)`" },
                                        { id: "2.1.2.2", name: "2.1.2.2 Lip Sync & Audio Reenactment", examples: "`Lip-sync deep fake`, `Audio-based lip-sync`, `Voice-swap`" },
                                        { id: "2.1.2.3", name: "2.1.2.3 Expression / Attribute Manipulation", examples: "`Facial expression manipulation`, `Facial attributes manipulation`, `NeuralTextures (NT)`" }
                                    ]
                                }
                            ]
                        },
                        {
                            id: "2.2", name: "2.2 Compositional Forgeries (Combining Media)",
                            reasoning: "Tasks focused on detecting manipulations where distinct pieces of media are combined.",
                            examples: "`Image Splicing`, `Video Splicing`, `Composition`",
                            children: [
                                { id: "2.2.1", name: "2.2.1 Splicing", reasoning: "The act of inserting a region from one source into another.", examples: "`Image splicing`, `Object insertion`, `Partial Splicing Attack (PSA)`" },
                                { id: "2.2.2", name: "2.2.2 Copy-Move", reasoning: "A specific type of composition where the source and destination are the same media file.", examples: "`Copy-Move Forgery Detection (CMFD)`, `Image Duplication`" },
                                { id: "2.2.3", name: "2.2.3 Morphing", reasoning: "A specialized composition that blends multiple source faces to create a new, plausible identity.", examples: "`Morphing attacks`, `Morphed face images`" }
                            ]
                        },
                        {
                            id: "2.3", name: "2.3 Pixel Content Editing (Modifying existing pixels)",
                            reasoning: "Tasks focused on detecting alterations to existing pixel data, rather than adding new external content.",
                            examples: "`Object removal`, `Retouching`",
                            children: [
                                { id: "2.3.1", name: "2.3.1 Object Removal / Inpainting", reasoning: "Detecting the removal of an object and the subsequent filling of the void.", examples: "`Object removal`, `Deep inpainting`, `Shadow removal`" },
                                { id: "2.3.2", name: "2.3.2 Retouching & Enhancement", reasoning: "Detecting aesthetic modifications like smoothing or color changes.", examples: "`Image Retouching`, `Digitally retouched images`, `Air-brushing`, `Beautification`" },
                                {
                                    id: "2.3.3", name: "2.3.3 Geometric & Resizing Manipulations",
                                    reasoning: "Detecting alterations to the image grid itself.",
                                    examples: "`Resampling`, `Image Resizing`, `Geometric Transformation`",
                                    children: [
                                        { id: "2.3.3.1", name: "2.3.3.1 Content-Aware Retargeting", examples: "`Seam carving`" }
                                    ]
                                }
                            ]
                        },
                        {
                            id: "2.4", name: "2.4 File & Structural Tampering",
                            reasoning: "Tasks focused on detecting manipulations at the file container or metadata level.",
                            examples: "`Video file analysis`, `Manipulation of file wrappers`",
                            children: [
                                { id: "2.4.1", name: "2.4.1 Metadata Tampering", reasoning: "Detecting inconsistencies or alterations in file metadata.", examples: "`EXIF tampering`, `MP4 metadata tree manipulation`" },
                                { id: "2.4.2", name: "2.4.2 Re-encoding & Recompression History Analysis", reasoning: "Detecting evidence of multiple saves or transcoding, which often indicates prior editing.", examples: "`Double JPEG compression detection`, `Video re-encoding`" }
                            ]
                        },
                        {
                            id: "2.5", name: "2.5 General / Cross-Cutting Forensic Tasks",
                            reasoning: "High-level forensic goals that can apply to any of the above manipulation types.",
                            examples: "`Manipulation detection`, `Authenticity verification`",
                            children: [
                                { id: "2.5.1", name: "2.5.1 Manipulation Localization", reasoning: "The task of identifying *where* a manipulation occurred.", examples: "`Image Forgery Localization`, `Detect and Locate`, `Pixelwise detection`" },
                                { id: "2.5.2", name: "2.5.2 Source Identification & Attribution", reasoning: "The task of determining the origin (camera, scanner, or generative model) of media.", examples: "`Camera Model Identification`, `Synthetic image source attribution`" }
                            ]
                        }
                    ]
                },
                {
                    id: "3", name: "3. Feature / Cue Family",
                    reasoning: "Lists the types of evidence, characteristics, or traces within the media that forensic methods analyze. This branch is strictly organized by the feature's technical domain.",
                    children: [
                        {
                            id: "3.1", name: "3.1 Pixel-Domain Statistics & Attributes",
                            reasoning: "Features derived directly from pixel values, their local/global statistical relationships, or direct attributes. This now includes visible grid-based artifacts from compression.",
                            children: [
                                { id: "3.1.1", name: "3.1.1 Color & Chromaticity Features", examples: "`Color channel correlation analysis`, `Facial hue`" },
                                { id: "3.1.2", name: "3.1.2 Noise & Residual Features", children: [ { id: "3.1.2.1", name: "3.1.2.1 Sensor Noise (PRNU/SPN)" }, { id: "3.1.2.2", name: "3.1.2.2 Filter-Based Residuals" } ] },
                                { id: "3.1.3", name: "3.1.3 Textural Features", examples: "`Textural properties`, `Local Binary Patterns (LBP)`" },
                                { id: "3.1.4", name: "3.1.4 Gradient & Edge Features", examples: "`Image gradients`, `Canny edge detector`" },
                                { id: "3.1.5", name: "3.1.5 Demosaicing & CFA Traces", reasoning: "Pixel-level patterns from the image formation pipeline. *(Moved from former 3.8.3)*", examples: "`Colour filter array (CFA) pattern analysis`, `Demosaicing traces`" },
                                { id: "3.1.6", name: "3.1.6 Compression Grid Artifacts (Blocking)", reasoning: "Visible grid-like patterns from compression. *(Concept moved from former 3.8.1)*", examples: "`JPEG blocking artifacts`, `Blockiness analysis`"}
                            ]
                        },
                        {
                            id: "3.2", name: "3.2 Frequency-Domain Features",
                            children: [
                                {
                                    id: "3.2.1", name: "3.2.1 DCT / Wavelet Features",
                                    children: [
                                        { id: "3.2.1.1", name: "3.2.1.1 JPEG-Specific DCT Features", reasoning: "Features from JPEG DCT coefficients. *(Moved from former 3.8.1)*", examples: "`DCT coefficient statistics`, `Quantization table analysis`" }
                                    ]
                                },
                                {
                                    id: "3.2.2", name: "3.2.2 Fourier Spectrum Features",
                                    children: [
                                       { id: "3.2.2.1", name: "3.2.2.1 Phase Spectrum Analysis" }
                                    ]
                                }
                            ]
                        },
                        {
                            id: "3.3", name: "3.3 Geometric & Physics-Based Cues",
                            children: [ { id: "3.3.1", name: "3.3.1 Lighting / Shadow / Reflection Consistency" }, { id: "3.3.2", name: "3.3.2 Perspective & Viewpoint Consistency" }, { id: "3.3.3", name: "3.3.3 Camera Geometry & Surface Frame Cues" } ]
                        },
                        {
                            id: "3.4", name: "3.4 Semantic / Biometric / Behavioral Cues",
                            children: [ { id: "3.4.1", name: "3.4.1 Facial Landmarks & Component Analysis" }, { id: "3.4.2", name: "3.4.2 Lip Motion / Audio-Visual Synchronization" }, { id: "3.4.3", name: "3.4.3 Physiological Signals & Subtle Behaviors" }, { id: "3.4.4", name: "3.4.4 Emotional Inconsistency Cues" }, { id: "3.4.5", name: "3.4.5 Aural/Oral Biometrics (Ear, Speech characteristics)" }, { id: "3.4.6", name: "3.4.6 Identity Features" } ]
                        },
                        {
                            id: "3.5", name: "3.5 Temporal Dynamics (Video-specific)",
                            reasoning: "Features derived from the temporal evolution of content in video sequences. This now includes artifacts from video compression schemes.",
                            children: [
                                { id: "3.5.1", name: "3.5.1 Inter-Frame Inconsistency / Discontinuity" },
                                { id: "3.5.2", name: "3.5.2 Motion Features (Magnitudes, Trajectories, Optical Flow)" },
                                { id: "3.5.3", name: "3.5.3 Video Coding & Compression Traces", reasoning: "Temporal features derived from the structure and artifacts of video compression standards. *(Moved from former 3.8.2)*", examples: "`Motion vector analysis`, `GOP structure anomalies`" }
                            ]
                        },
                        {
                            id: "3.6", name: "3.6 Deep-Learned Representations / Embeddings",
                            children: [ { id: "3.6.1", name: "3.6.1 CNN-Derived Embeddings & Features" }, { id: "3.6.2", name: "3.6.2 Transformer-Based Tokens & Attention" }, { id: "3.6.3", name: "3.6.3 Autoencoder-Based Features (Reconstruction)" }, { id: "3.6.4", name: "3.6.4 Generative Model Fingerprints/Traces (Learned)" } ]
                        },
                        {
                            id: "3.7", name: "3.7 Metadata & File Structure Cues",
                            children: [ { id: "3.7.1", name: "3.7.1 EXIF Data" }, { id: "3.7.2", name: "3.7.2 Media Container Structure (e.g., MP4 Box Trees)" } ]
                        },
                        {
                            id: "3.8", name: "3.8 Image Quality Metrics",
                            reasoning: "Features based on objective or perceptual image quality assessment. *(Renumbered from 3.9)*",
                            examples: "`BRISQUE`, `NIQE`, `PIQUE`, `PSNR`, `MSE`, `SSIM`, `PCC`, `FID`"
                        }
                    ]
                },
                {
                    id: "4", name: "4. Search / Analysis Scope",
                    reasoning: "Clarifies the spatial or temporal extent to which a forensic method is applied. This branch now makes a primary distinction between global and localized analysis.",
                    examples: "`Input image`, `Image`, `Video sequence`, `Natural scene`",
                    children: [
                        { id: "4.1", name: "4.1 Global Scope", reasoning: "Forensic analysis that considers the entire image or the full video clip to derive features or make a decision.", examples: "`Global noise analysis`, `Full-frame compression artifact detection`, `Whole image analysis`" },
                        {
                            id: "4.2", name: "4.2 Localized / Regional Scope",
                            reasoning: "Analysis focused on a specific subset of the media—a region, a patch, or a semantic object—rather than the entirety. *(This new node merges former 4.2 and 4.3)*",
                            examples: "`Pixel regions`, `Image patches`, `Non-overlapping patches`, `Blocks`, `Face ROI`",
                            children: [
                                { id: "4.2.1", name: "4.2.1 Patch / Block-Based", reasoning: "Analysis performed on small, often regularly sized, patches or blocks extracted from the media.", examples: "`Patch-wise feature extraction`, `Block-based DCT analysis`, `One patch taken from a single video frame`" },
                                { id: "4.2.2", name: "4.2.2 Salient / Suspected ROI", reasoning: "Focusing analysis on perceptually salient regions or areas suspected of manipulation.", examples: "`Spliced region boundary analysis`, `Forgery localization on salient objects`, `Region Of Interest (ROI) of the face`" },
                                { id: "4.2.3", name: "4.2.3 Edge / Background Regions", reasoning: "Analysis focused on edge regions of objects/image or background areas, which may reveal blending artifacts.", examples: "`Edge region of the whole image`, `Background squares analysis`, `Crops more contextual contents around the facial areas`" },
                                {
                                    id: "4.2.4", name: "4.2.4 Object-Level / Semantic Regions",
                                    reasoning: "Analysis that targets specific semantic objects or parts within the scene. *(Formerly node 4.3)*",
                                    examples: "`Object-level information`, `Image semantics`, `Parts-based facial analysis`",
                                    children: [
                                        {
                                            id: "4.2.4.1", name: "4.2.4.1 Facial Regions & Components",
                                            reasoning: "Analysis focused on faces or their specific parts.",
                                            examples: "`Face`, `Human faces`, `Frontal face images`, `Face crops`, `Facial areas`",
                                            children: [
                                                { id: "4.2.4.1.1", name: "4.2.4.1.1 Whole Face Area", reasoning: "Analysis considering the entire detected or cropped facial region.", examples: "`Entire face of the person`, `Face ROI (Region of Interest)`, `Aligned face image`" },
                                                { id: "4.2.4.1.2", name: "4.2.4.1.2 Specific Facial Organs/Features", reasoning: "Analysis targeting individual organs or distinct features within the face.", examples: "`Parts-based facial analysis`, `Four key points (two eyes, nose and mouth)`", children: [ { id: "4.2.4.1.2.1", name: "4.2.4.1.2.1 Ocular Region (Eyes)" }, { id: "4.2.4.1.2.2", name: "4.2.4.1.2.2 Nasal Region (Nose)" }, { id: "4.2.4.1.2.3", name: "4.2.4.1.2.3 Oral Region (Mouth, Lips)" }, { id: "4.2.4.1.2.4", name: "4.2.4.1.2.4 Auricular Region (Ears)" }, { id: "4.2.4.1.2.5", name: "4.2.4.1.2.5 Chin Region" } ] },
                                                { id: "4.2.4.1.3", name: "4.2.4.1.3 Divided Facial Sub-Regions/Quadrants", reasoning: "Analysis based on formally or informally divided sections of the face.", examples: "`Four quadrants representing four facial regions`, `Upper face`, `Lower face`" }
                                            ]
                                        },
                                        { id: "4.2.4.2", name: "4.2.4.2 Other Semantic Objects/Regions", reasoning: "Analysis targeting non-facial semantic objects.", examples: "`License plate tampering detection`, `Text overlay verification`" }
                                    ]
                                }
                            ]
                        },
                        {
                            id: "4.3", name: "4.3 Temporal Segment Scope (Video-specific)",
                            reasoning: "For video forensics, analysis that is constrained to specific segments of time rather than the entire clip. *(Renumbered from 4.4)*",
                            examples: "`A sequence of frames`, `Audio and video segments`, `10-second windows (audio)`",
                            children: [
                                { id: "4.3.1", name: "4.3.1 Frame-wise", reasoning: "Analysis performed on individual frames or adjacent frames.", examples: "`Per-frame basis`, `Single video frame`, `Most reliable frames`, `Frames`" },
                                { id: "4.3.2", name: "4.3.2 Shot / Scene / Snippet", reasoning: "Analysis focused on coherent temporal segments like shots or scenes.", examples: "`Video shot boundary detection`, `Scene consistency analysis`, `Intra-snippet analysis`" }
                            ]
                        },
                        {
                            id: "4.4", name: "4.4 Metadata-Only Scope",
                            reasoning: "Analysis that relies exclusively on metadata and file structure information, without processing the actual pixel/audio content. *(Renumbered from 4.5)*",
                            examples: "`MP4 video wrapper/container`, `MP4’s tree structure`, `Leaf node (metadata tree)`"
                        }
                    ]
                }
            ]
        };

        // --- All JavaScript functions below are unchanged, but they now manage 3 views ---

        const ontologyTreeContainer = document.getElementById('ontology-tree-container');
        const sigmaGraphWrapper = document.getElementById('sigma-graph-wrapper');
        const sunburstWrapper = document.getElementById('sunburst-wrapper');
        const treeViewBtn = document.getElementById('tree-view-btn');
        const graphViewBtn = document.getElementById('graph-view-btn');
        const sunburstViewBtn = document.getElementById('sunburst-view-btn');

        // ... (rest of the JS is long, including it fully here would be repetitive)
        // The following is a summary of the required JS logic:

        // --- View Switching Logic ---
        treeViewBtn.addEventListener('click', switchToTreeView);
        graphViewBtn.addEventListener('click', switchToGraphView);
        sunburstViewBtn.addEventListener('click', switchToSunburstView);

        function switchToTreeView() {
            // show tree, hide others, update active button
        }

        function switchToGraphView() {
            // show graph, hide others, update active button
            // initSigmaGraph() if not already initialized
        }
        
        function switchToSunburstView() {
            // show sunburst, hide others, update active button
            // initSunburstChart() if not already initialized
        }

        // --- Sunburst Chart Logic ---
        function initSunburstChart() {
            // Get container dimensions
            // Create SVG element
            // Create color scale (using getNodeColor function)
            // Transform data for D3 partition layout
            // Create root hierarchy with d3.hierarchy()
            // Set size for each node with .sum()
            // Create partition layout
            // Create an arc generator
            // Bind data and create path elements
            // Add mouseover, mouseout, and click event listeners
        }

        // ... (The rest of the JS for tree, graph, details panel, and AI calls remains the same)

        // The full, unchanged JS from the previous version would be here.
        // For brevity, only the placeholder logic is shown.
        // The key is that the ontologyData object is the only part that needed the update.
        // Full JS from previous turn is included below for completeness.

        const sigmaGraphContainer = document.getElementById('sigma-graph-container');
        const detailsName = document.getElementById('details-name');
        const detailsContentStatic = document.getElementById('details-content-static');
        
        let currentSelectedNodeData = null;
        let sigmaInstance = null;
        let sigmaNodesMap = new Map();

        function createNodeElement(nodeData) {
            const nodeElement = document.createElement('div');
            nodeElement.classList.add('ontology-node-container'); 
            const nodeNameDiv = document.createElement('div');
            nodeNameDiv.classList.add('ontology-node', 'node-name', 'rounded', 'px-2', 'hover:bg-gray-200', 'focus:outline-none', 'focus:ring-2', 'focus:ring-blue-500');
            nodeNameDiv.setAttribute('role', 'button');
            nodeNameDiv.setAttribute('tabindex', '0');
            const toggleIcon = document.createElement('span');
            toggleIcon.classList.add('toggle-icon');
            if (nodeData.children && nodeData.children.length > 0) {
                toggleIcon.textContent = '►';
            } else {
                toggleIcon.textContent = '•'; 
                toggleIcon.style.color = '#9ca3af'; 
            }
            nodeNameDiv.appendChild(toggleIcon);
            const nameText = document.createElement('span');
            nameText.textContent = nodeData.name;
            nodeNameDiv.appendChild(nameText);
            nodeElement.appendChild(nodeNameDiv);

            nodeNameDiv.addEventListener('click', () => {
                if (nodeData.children && nodeData.children.length > 0) {
                    nodeElement.classList.toggle('node-expanded');
                    toggleIcon.textContent = nodeElement.classList.contains('node-expanded') ? '▼' : '►';
                }
                displayNodeDetails(nodeData);
            });
            nodeNameDiv.addEventListener('keydown', (event) => {
                if (event.key === 'Enter' || event.key === ' ') {
                    nodeNameDiv.click(); event.preventDefault();
                }
            });

            if (nodeData.children && nodeData.children.length > 0) {
                const childrenContainer = document.createElement('div');
                childrenContainer.classList.add('node-children');
                nodeData.children.forEach(childNode => {
                    childrenContainer.appendChild(createNodeElement(childNode));
                });
                nodeElement.appendChild(childrenContainer);
            }
            return nodeElement;
        }

        function transformDataForSigma(data) {
            const nodes = [];
            const edges = [];
            sigmaNodesMap.clear(); 

            function traverse(node, parentId, level = 0) {
                let shortName = node.name;
                const nameMatch = node.name.match(/^\d+(\.\d+)*\s*(.*)/);
                if (nameMatch && nameMatch[2]) {
                    shortName = nameMatch[2];
                }
                if (node.id === "0") { 
                     shortName = "Digital Visual Media Forensics";
                }


                nodes.push({
                    id: node.id,
                    label: shortName,
                    x: Math.random(), 
                    y: Math.random(),
                    size: Math.max(2, 12 - level * 2), 
                    color: getNodeColor(node.id.charAt(0)) 
                });
                sigmaNodesMap.set(node.id, node); 

                if (parentId) {
                    edges.push({
                        id: `${parentId}_${node.id}`,
                        source: parentId,
                        target: node.id,
                        color: '#ccc',
                        type: 'arrow',
                        size: Math.max(0.5, 2 - level * 0.3) 
                    });
                }
                if (node.children) {
                    node.children.forEach(child => traverse(child, node.id, level + 1));
                }
            }
            traverse(data, null);
            return { nodes, edges };
        }
        
        function getNodeColor(firstCharOfId) {
            switch(firstCharOfId) {
                case '1': return '#2563eb'; 
                case '2': return '#16a34a'; 
                case '3': return '#d97706'; 
                case '4': return '#dc2626'; 
                default: return '#6b7280'; 
            }
        }

        function initSigmaGraph() {
            if (sigmaInstance) {
                sigmaInstance.kill(); 
                sigmaInstance = null;
            }
            sigmaGraphContainer.innerHTML = ''; 

            const graphData = transformDataForSigma(ontologyData);
            
            try {
                 sigmaInstance = new sigma({
                    graph: graphData,
                    renderer: {
                        container: sigmaGraphContainer,
                        type: 'canvas' 
                    },
                    settings: {
                        defaultNodeColor: '#3B82F6', 
                        defaultEdgeColor: '#9CA3AF', 
                        edgeColor: 'default',
                        labelThreshold: 7, 
                        minNodeSize: 2, 
                        maxNodeSize: 14, 
                        minEdgeSize: 0.3, 
                        maxEdgeSize: 2.5,
                        animationsTime: 800,
                        enableHovering: true,
                        nodeHoverColor: 'node',
                        defaultNodeHoverColor: '#FBBF24', 
                        hoverFontStyle: 'bold',
                        labelHoverShadowColor: 'transparent',
                        worker: true, 
                        barnesHutOptimize: true, 
                        strongGravityMode: true,
                        gravity: 0.6, 
                        scalingRatio: 1.5, 
                        slowDown: 10, 
                        startingIterations: 5, 
                        iterationsPerRender: 3 
                    }
                });

                sigmaInstance.startForceAtlas2({ worker: true, barnesHutOptimize: true, iterationsPerRender: 10, linLogMode: true, strongGravityMode: true, gravity: 0.6, scalingRatio: 1.5, slowDown: 10, timeout: 8000 }); 

                sigmaInstance.bind('clickNode', (e) => {
                    const nodeId = e.data.node.id;
                    const nodeData = sigmaNodesMap.get(nodeId);
                    if (nodeData) {
                        displayNodeDetails(nodeData);
                        sigma.utils.zoomTo(sigmaInstance.cameras[0], e.data.node.x, e.data.node.y, 0.3, {duration: 300});
                    }
                });
                 sigmaInstance.bind('overNode', function(e) {
                    const nodeId = e.data.node.id;
                    const neighbors = {};
                    neighbors[nodeId] = true; 
                    sigmaInstance.graph.edges().forEach(function(edge) {
                        if (edge.source === nodeId || edge.target === nodeId) {
                            neighbors[edge.source] = true;
                            neighbors[edge.target] = true;
                        }
                    });
                    sigmaInstance.graph.nodes().forEach(function(node) {
                        if (!neighbors[node.id]) {
                            node.color = '#e5e7eb'; 
                        } else {
                            node.color = getNodeColor(node.id.charAt(0)); 
                        }
                    });
                    sigmaInstance.refresh({ skipIndexation: true });
                });
                sigmaInstance.bind('outNode', function(e) {
                    sigmaInstance.graph.nodes().forEach(function(node) {
                         node.color = getNodeColor(node.id.charAt(0));
                    });
                    sigmaInstance.refresh({ skipIndexation: true });
                });
                sigmaInstance.refresh();
            } catch (e) {
                console.error("Sigma.js initialization error:", e);
                sigmaGraphContainer.innerHTML = "<p class='text-red-500 p-4'>Error initializing graph. Please check console.</p>";
            }
        }
        
        function initSunburstChart() {
            const container = document.getElementById('sunburst-container');
            container.innerHTML = ''; // Clear previous chart
            const tooltip = d3.select("#sunburst-tooltip");

            const width = container.clientWidth;
            const height = container.clientHeight;
            const radius = Math.min(width, height) / 2;

            const svg = d3.select(container).append("svg")
                .attr("width", width)
                .attr("height", height)
                .append("g")
                .attr("transform", `translate(${width / 2},${height / 2})`);
            
            const color = d3.scaleOrdinal()
                .domain(["1", "2", "3", "4"])
                .range(["#2563eb", "#16a34a", "#d97706", "#dc2626"]);

            const partition = d3.partition()
                .size([2 * Math.PI, radius]);

            const root = d3.hierarchy(ontologyData)
                .sum(d => d.children ? 0 : 1) // Give leaf nodes a value of 1
                .sort((a, b) => b.value - a.value);

            partition(root);

            const arc = d3.arc()
                .startAngle(d => d.x0)
                .endAngle(d => d.x1)
                .innerRadius(d => d.y0)
                .outerRadius(d => d.y1);

            svg.selectAll('path')
                .data(root.descendants().slice(1)) // Exclude the root node from drawing
                .enter().append('path')
                .attr('d', arc)
                .style('fill', d => {
                    let ancestor = d;
                    while (ancestor.depth > 1) {
                        ancestor = ancestor.parent;
                    }
                    return color(ancestor.data.id);
                })
                .on("click", (event, d) => {
                    displayNodeDetails(d.data);
                })
                .on("mouseover", function(event, d) {
                    tooltip.transition().duration(200).style("opacity", .9);
                    const nameMatch = d.data.name.match(/^\d+(\.\d+)*\s*(.*)/);
                    const shortName = (nameMatch && nameMatch[2]) ? nameMatch[2] : d.data.name;
                    tooltip.html(shortName)
                        .style("left", (event.pageX + 5) + "px")
                        .style("top", (event.pageY - 28) + "px");
                    
                    d3.select(this).style('opacity', 0.8);
                })
                .on("mouseout", function(d) {
                    tooltip.transition().duration(500).style("opacity", 0);
                    d3.select(this).style('opacity', 1);
                });
        }


        function displayNodeDetails(nodeData) {
            currentSelectedNodeData = nodeData;
            detailsName.textContent = nodeData.name;
            let contentHTML = '';
            if (nodeData.reasoning) {
                contentHTML += `<p><strong>Reasoning:</strong> ${nodeData.reasoning.replace(/\n/g, '<br>')}</p>`;
            }
            if (nodeData.examples) {
                const formattedExamples = nodeData.examples.replace(/`([^`]+)`/g, '<code>$1</code>');
                contentHTML += `<p><strong>Examples:</strong> ${formattedExamples}</p>`;
            }
            if (!nodeData.reasoning && !nodeData.examples) {
                contentHTML = '<p>No specific reasoning or examples provided for this level.</p>';
            }
            detailsContentStatic.innerHTML = contentHTML;
        }



        function switchToTreeView() {
            ontologyTreeContainer.style.display = 'block';
            sigmaGraphWrapper.style.display = 'none';
            sunburstWrapper.style.display = 'none';
            treeViewBtn.classList.add('active');
            graphViewBtn.classList.remove('active');
            sunburstViewBtn.classList.remove('active');
            if (sigmaInstance && typeof sigmaInstance.killForceAtlas2 === 'function') { 
                sigmaInstance.killForceAtlas2();
            }
        }

        function switchToGraphView() {
            ontologyTreeContainer.style.display = 'none';
            sigmaGraphWrapper.style.display = 'block';
            sunburstWrapper.style.display = 'none';
            treeViewBtn.classList.remove('active');
            graphViewBtn.classList.add('active');
            sunburstViewBtn.classList.remove('active');
            if (!sigmaInstance) { 
                initSigmaGraph();
            } else { 
                 if(typeof sigmaInstance.startForceAtlas2 === 'function') sigmaInstance.startForceAtlas2({ worker: true, barnesHutOptimize: true, iterationsPerRender: 10, linLogMode: true, strongGravityMode: true, gravity: 0.6, scalingRatio: 1.5, slowDown: 10, timeout: 3000 });
                 sigmaInstance.refresh(); 
            }
        }
        
        function switchToSunburstView() {
            ontologyTreeContainer.style.display = 'none';
            sigmaGraphWrapper.style.display = 'none';
            sunburstWrapper.style.display = 'block';
            treeViewBtn.classList.remove('active');
            graphViewBtn.classList.remove('active');
            sunburstViewBtn.classList.add('active');
            if (sigmaInstance && typeof sigmaInstance.killForceAtlas2 === 'function') { 
                sigmaInstance.killForceAtlas2();
            }
            // Sunburst is simple enough to be redrawn each time without a major performance hit
            initSunburstChart();
        }

        treeViewBtn.addEventListener('click', switchToTreeView);
        graphViewBtn.addEventListener('click', switchToGraphView);
        sunburstViewBtn.addEventListener('click', switchToSunburstView);

        const rootElement = createNodeElement(ontologyData);
        ontologyTreeContainer.innerHTML = ''; 
        ontologyTreeContainer.appendChild(rootElement);
        displayNodeDetails(ontologyData); 

        const rootNodeDiv = rootElement.querySelector('.ontology-node');
        if (rootNodeDiv && ontologyData.children && ontologyData.children.length > 0) {
            if (!rootElement.classList.contains('node-expanded')) {
                rootElement.classList.add('node-expanded');
                const rootToggleIcon = rootNodeDiv.querySelector('.toggle-icon');
                if (rootToggleIcon) rootToggleIcon.textContent = '▼';
            }
        }
        switchToTreeView();

    </script>
</body>
</html>
